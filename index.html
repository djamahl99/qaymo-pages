<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description" content="Box-QAymo: A box-referring VQA dataset for evaluating vision-language models on spatial and temporal reasoning in autonomous driving scenarios.">
  <meta property="og:title" content="Box-QAymo: Box-Referring VQA Dataset for Autonomous Driving"/>
  <meta property="og:description" content="A hierarchical evaluation framework for VLMs featuring crowd-sourced semantic annotations, box-referring questions, and temporal reasoning tasks."/>
  <meta property="og:url" content="https://djamahl99.github.io/qaymo-pages/"/>
  <meta property="og:image" content="static/images/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>

  <meta name="twitter:title" content="Box-QAymo: Box-Referring VQA Dataset"/>
  <meta name="twitter:description" content="Hierarchical VQA evaluation for autonomous driving with crowd-sourced semantics and temporal reasoning."/>
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <meta name="keywords" content="VQA, Visual Question Answering, Autonomous Driving, Vision-Language Models, Box-Referring, Temporal Reasoning, Dataset, Benchmark">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Box-QAymo | VQA Dataset for Autonomous Driving</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/bulma/0.9.4/css/bulma.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css">
  
  <style>
    .publication-title {
      font-size: 2.5rem;
      font-weight: 700;
      line-height: 1.2;
      margin-bottom: 1rem;
    }
    
    .publication-authors {
      margin-bottom: 1rem;
    }
    
    .author-block {
      margin-right: 1rem;
    }
    
    .publication-links {
      margin-top: 1.5rem;
    }
    
    .button {
      margin: 0.25rem;
    }
    
    .hero-body {
      padding: 3rem 1.5rem;
    }
    
    .section {
      padding: 3rem 1.5rem;
    }
    
    .content {
      line-height: 1.7;
    }
    
    .content p {
      margin-bottom: 1.5rem;
    }
    
    .stats-grid {
      display: grid;
      grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
      gap: 1.5rem;
      margin: 2rem 0;
    }
    
    .stat-box {
      background: #f8f9fa;
      border: 2px solid #e9ecef;
      border-radius: 8px;
      padding: 1.5rem;
      text-align: center;
    }
    
    .stat-number {
      font-size: 2.5rem;
      font-weight: 700;
      color: #3273dc;
    }
    
    .stat-label {
      font-size: 1rem;
      color: #666;
      margin-top: 0.5rem;
    }
    
    .methodology-section {
      background: #f8f9fa;
      border-radius: 12px;
      padding: 2rem;
      margin: 2rem 0;
    }
    
    .stage-grid {
      display: grid;
      grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
      gap: 1.5rem;
      margin-top: 1.5rem;
    }
    
    .stage-box {
      background: white;
      border: 2px solid #e9ecef;
      border-radius: 8px;
      padding: 1.5rem;
      transition: all 0.3s ease;
    }
    
    .stage-box:hover {
      border-color: #3273dc;
      box-shadow: 0 4px 12px rgba(50, 115, 220, 0.15);
    }
    
    .stage-header {
      display: flex;
      align-items: center;
      margin-bottom: 1rem;
    }
    
    .stage-number {
      width: 40px;
      height: 40px;
      background: linear-gradient(135deg, #3273dc, #667eea);
      color: white;
      border-radius: 50%;
      display: flex;
      align-items: center;
      justify-content: center;
      font-weight: bold;
      margin-right: 1rem;
    }
    
    .stage-title {
      font-size: 1.25rem;
      font-weight: 600;
      color: #333;
    }
    
    .stage-items {
      list-style: none;
      margin: 0;
      padding: 0;
    }
    
    .stage-items li {
      padding: 0.25rem 0;
      color: #666;
      position: relative;
      padding-left: 1rem;
    }
    
    .stage-items li::before {
      content: 'â–¸';
      color: #3273dc;
      font-weight: bold;
      position: absolute;
      left: 0;
    }
    
    .question-types {
      display: grid;
      grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
      gap: 1rem;
      margin: 1.5rem 0;
    }
    
    .question-type {
      background: white;
      border: 2px solid #e9ecef;
      border-radius: 8px;
      padding: 1rem;
    }
    
    .type-binary { border-left: 4px solid #2196f3; }
    .type-attribute { border-left: 4px solid #4caf50; }
    .type-motion { border-left: 4px solid #9c27b0; }
    
    .question-type h4 {
      font-size: 1.1rem;
      font-weight: 600;
      margin-bottom: 0.5rem;
    }
    
    .question-type p {
      font-size: 0.9rem;
      color: #666;
      margin: 0;
    }
    
    .performance-grid {
      display: grid;
      grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
      gap: 1.5rem;
      margin: 1.5rem 0;
    }
    
    .performance-box {
      background: white;
      border: 2px solid #e9ecef;
      border-radius: 8px;
      padding: 1.5rem;
    }
    
    .performance-title {
      font-size: 1.2rem;
      font-weight: 600;
      margin-bottom: 1rem;
      color: #333;
    }
    
    .model-result {
      display: flex;
      justify-content: space-between;
      padding: 0.5rem 0;
      border-bottom: 1px solid #f0f0f0;
    }
    
    .model-result:last-child {
      border-bottom: none;
    }
    
    .model-name {
      font-weight: 500;
    }
    
    .f1-score {
      font-weight: 600;
      color: #3273dc;
    }
    
    .code-section {
      background: #f8f9fa;
      border-radius: 8px;
      padding: 1.5rem;
      margin: 1.5rem 0;
    }
    
    .code-block {
      background: #2d3748;
      color: #e2e8f0;
      padding: 1rem;
      border-radius: 4px;
      font-family: 'Courier New', monospace;
      font-size: 0.9rem;
      overflow-x: auto;
    }
    
    @media (max-width: 768px) {
      .publication-title {
        font-size: 2rem;
      }
      
      .stats-grid {
        grid-template-columns: repeat(auto-fit, minmax(150px, 1fr));
      }
      
      .stage-grid {
        grid-template-columns: 1fr;
      }
    }
  </style>
</head>

<body>
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title publication-title">Box-QAymo: Box-Referring VQA Dataset for Autonomous Driving</h1>
            
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://djamahl99.github.io" target="_blank">Djamahl Etchegaray</a>,
              </span>
              <span class="author-block">
                <a href="#" target="_blank">Yuxia Fu</a>,
              </span>
              <span class="author-block">
                <a href="#" target="_blank">Zi Huang</a>,
              </span>
              <span class="author-block">
                <a href="#" target="_blank">Yadan Luo</a>
              </span>
            </div>
            
            <div class="is-size-6 publication-authors">
              <p>UQMM Lab, University of Queensland, Brisbane, Australia</p>
                          </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <span class="link-block">
                  <a href="static/pdfs/WaymoVQA_Dataset.pdf" target="_blank" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>

                <span class="link-block">
                  <a href="https://github.com/djamahl99/box-qaymo" target="_blank" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Abstract -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Interpretable communication is essential for safe and trustworthy autonomous driving, yet current vision-language models (VLMs) often operate under idealized assumptions and struggle to capture user intent in real-world scenarios. Existing driving-oriented VQA datasets are limited to full-scene descriptions or waypoint prediction, preventing the assessment of whether VLMs can respond to localized user-driven queries.
            </p>
            <p>
              We introduce <strong>Box-QAymo</strong>, a box-referring dataset and benchmark designed to both evaluate and finetune VLMs on spatial and temporal reasoning over user-specified objects. Users express intent by drawing bounding boxes, offering a fast and intuitive interface for focused queries in complex scenes.
            </p>
            <p>
              Specifically, we propose a hierarchical evaluation protocol that begins with binary sanity-check questions to assess basic model capacities, and progresses to (1) attribute prediction for box-referred objects, (2) motion understanding of target instances, and (3) spatiotemporal motion reasoning over inter-object dynamics across frames.
            </p>
            <p>
              To support this, we crowd-sourced fine-grained object classes and visual attributes that reflect the complexity drivers encounter, and extract object trajectories to construct temporally grounded QA pairs. Rigorous quality control through negative sampling, temporal consistency checks, and difficulty-aware balancing guarantee dataset robustness and diversity.
            </p>
            <p>
              Our comprehensive evaluation reveals significant limitations in current VLMs when queried about perception questions, highlighting the gap in achieving real-world performance. This work provides a foundation for developing more robust and interpretable autonomous driving systems that can communicate effectively with users under real-world conditions.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Dataset Statistics -->
  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Dataset Statistics</h2>
          <div class="stats-grid">
            <div class="stat-box">
              <div class="stat-number">20,779</div>
              <div class="stat-label">Total Questions</div>
            </div>
            <div class="stat-box">
              <div class="stat-number">1,662</div>
              <div class="stat-label">Binary Questions</div>
            </div>
            <div class="stat-box">
              <div class="stat-number">5,403</div>
              <div class="stat-label">Attribute Questions</div>
            </div>
            <div class="stat-box">
              <div class="stat-number">13,714</div>
              <div class="stat-label">Motion Questions</div>
            </div>
            <div class="stat-box">
              <div class="stat-number">202</div>
              <div class="stat-label">Scenes</div>
            </div>
            <div class="stat-box">
              <div class="stat-number">50%</div>
              <div class="stat-label">Objects Annotated</div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Dataset Pipeline Overview -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <h2 class="title is-3 has-text-centered">Box-QAymo Dataset Pipeline</h2>
      
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <img src="static/images/QAymo.png" style="width: 100%; border-radius: 8px; box-shadow: 0 4px 12px rgba(0,0,0,0.1);">
        </div>
      </div>
      
      <div class="content has-text-centered" style="margin-top: 1.5rem;">
        <p class="is-size-6" style="color: #666; font-style: italic;">
          <strong>Overview of the Box-QAymo dataset pipeline for evaluating vision-language models (VLMs).</strong> 
          Step 1 extracts 3D metadata from Waymo which is enhanced with human-annotated semantics. 
          Step 2 introduces box-referenced visual question answering (VQA) tasks spanning instance recognition, motion interpretation, and temporal trajectory reasoning. 
          Step 3 implements rigorous quality control through negative sampling, temporal consistency filtering, and difficulty-aware balancing to ensure a robust and challenging dataset. 
          Step 4 benchmarks general and domain-specific VLMs in zero-shot and fine-tuned settings.
        </p>
      </div>
    </div>
  </section>

<!-- Question Types -->
<section class="section">
    <div class="container is-max-desktop">
      <h2 class="title is-3 has-text-centered">Question Categories</h2>
      
      <div class="question-types">
        <div class="question-type type-binary">
          <h4>Binary Questions</h4>
          <p><strong>Movement Status:</strong> "Are there any stationary vehicles?"</p>
          <p><strong>Orientation:</strong> "Are there any vehicles moving towards the camera?"</p>
        </div>

        <div class="question-type type-attribute">
          <h4>Attribute Questions</h4>
          <p><strong>Fine-grained Classification:</strong> "What type of object is in the red box?"</p>
          <p><strong>Color Recognition:</strong> "What color is the object highlighted in red?"</p>
          <p><strong>Facing Direction:</strong> "What direction is the object in the red box facing?"</p>
        </div>

        <div class="question-type type-motion">
          <h4>Motion Questions</h4>
          <p><strong>Speed Assessment:</strong> "How fast is the blue sedan moving?"</p>
          <p><strong>Movement Direction:</strong> "What direction is the object in the red box moving?"</p>
          <p><strong>Relative Motion Analysis:</strong> "Is the green pickup truck traveling faster than the ego vehicle?"</p>
          <p><strong>Traffic Element Recognition:</strong> "Is the ego vehicle approaching a stop sign?"</p>
          <p><strong>Trajectory Analysis:</strong> "Are the ego vehicle and the truck on a collision course?"</p>
          <p><strong>Relative Motion Direction:</strong> "What is the relative motion direction of the hatchback compared to ego?"</p>
          <p><strong>Path Conflict Detection:</strong> "Is there a vehicle in the ego vehicle's future path?"</p>
        </div>
      </div>
    </div>
  </section>

  <!-- Code & Framework -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <h2 class="title is-3 has-text-centered">Code & Framework</h2>
      
      <div class="content">
        <p>
          Box-QAymo provides a comprehensive framework for generating, processing, and evaluating visual question answering (VQA) tasks on the Waymo dataset. The framework supports diverse question types, multiple evaluation metrics, and various answer formats.
        </p>

        <div class="columns is-multiline">
          <div class="column is-6">
            <h3>Core Components</h3>
            <ul>
              <li><strong>Data Processing:</strong> Waymo dataset extraction and preprocessing</li>
              <li><strong>Question Generation:</strong> Hierarchical prompt generators for different question types</li>
              <li><strong>Model Evaluation:</strong> Support for multiple VLMs and evaluation metrics</li>
              <li><strong>Answer Processing:</strong> Handles multiple choice, text, and bounding box answers</li>
            </ul>
          </div>
          
          <div class="column is-6">
            <h3>Supported Models</h3>
            <ul>
              <li><strong>VLMs:</strong> LLaVA, Qwen-VL, SENNA</li>
              <li><strong>Evaluation Metrics:</strong> F1, Precision, Recall</li>
              <li><strong>Question Types:</strong> Binary, attribute, and motion reasoning</li>
            </ul>
          </div>
        </div>

        <div class="has-text-centered" style="margin-top: 2rem;">
          <a href="https://github.com/djamahl99/box-qaymo" target="_blank" class="button is-primary is-large">
            <span class="icon">
              <i class="fab fa-github"></i>
            </span>
            <span>View Code & Setup Instructions</span>
          </a>
        </div>

        <div style="margin-top: 2rem; padding: 1.5rem; background: #f8f9fa; border-radius: 8px;">
          <h3>Quick Start</h3>
          <p>
            For detailed setup instructions, including Waymo dataset preprocessing, crowd-sourced metadata download, 
            and model evaluation scripts, please visit our GitHub repository. The repository includes:
          </p>
          <ul>
            <li>Complete installation and setup guide</li>
            <li>Waymo dataset extraction scripts</li>
            <li>VQA dataset generation pipeline</li>
            <li>Model evaluation and comparison tools</li>
            <li>Pre-trained model integration</li>
          </ul>
        </div>
      </div>
    </div>
  </section>

  <!-- Results -->
  <section class="section">
    <div class="container is-max-desktop">
      <h2 class="title is-3 has-text-centered">Results</h2>

      <div class="content">
        <h3>Key Findings</h3>
        <ul>
          <li><strong>Hierarchical Complexity:</strong> Performance decreases from binary (66.1%) to attribute (18.3%) to motion (37.6%) questions, validating our complexity assumptions.</li>
          <li><strong>Box Grounding:</strong> Red bounding boxes consistently improve performance, with Qwen-VL showing +1.39% F1 improvement on average.</li>
          <li><strong>Temporal Reasoning:</strong> Counter-intuitively, two-frame inputs degrade performance compared to single frames, suggesting current VLMs struggle with short-term temporal integration.</li>
          <li><strong>Domain Specificity:</strong> Senna's poor performance despite being driving-specific reveals brittleness of narrow task training.</li>
        </ul>
      </div>
    </div>
  </section>

  <!-- Citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{etchegaray2024boxqaymo,
  title={Box-QAymo: Box-Referring VQA Dataset for Autonomous Driving},
  author={Etchegaray, Djamahl, Fu, Yuxia, Huang, Zi and Luo, Yadan},
  journal={arXiv preprint},
  year={2025}
}</code></pre>
    </div>
  </section>

  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a>. 
              This website is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>
</body>
</html>